{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie per lettura file\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Kfold and GridSearch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Grafici\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "# Data Analysis\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    precision_recall_curve, classification_report, confusion_matrix,\n",
    "    roc_curve, roc_auc_score, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# Modelli con Alberi\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "\n",
    "# Librerie Torch per MLP\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, SubsetRandomSampler\n",
    "from torchmetrics import F1Score, Accuracy, Precision, Recall\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Explainable AI (SHAP)\n",
    "import shap\n",
    "\n",
    "# Librerie per gestione dati parallela\n",
    "import modin.pandas as mpd\n",
    "import modin.config as cfg\n",
    "\n",
    "# Visualizzazione matrice di confusione\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import psutil\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg') \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(save_path, state_dict):\n",
    "    joblib.dump(state_dict, save_path)\n",
    "\n",
    "def load_state(save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        return joblib.load(save_path)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv(\"Preprocessing/drug_scores_with_targets.csv\")\n",
    "df_clean = pd.read_csv(\"Preprocessing/transcrittoma_pulito.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you can run models that predict drug sensitivity only. The analysis specifically focuses on the top 30 drugs, although you can include more if desired. However, note that drugs with a negative score perform worse than a random model and are therefore not recommended for useâ€”this applies especially to drugs ranked beyond the 194th position.\n",
    "\n",
    "The DL functions are implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, dim=128, dropout=0.2, learning_rate = 0.01):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates and compiles a simple multi-layer perceptron (MLP) model for binary classification.\n",
    "    \n",
    "    The architecture includes three hidden layers with ReLU activation, dropout for regularization, \n",
    "    and a final sigmoid output layer. Uses Adam optimizer with specified learning rate.\n",
    "    \n",
    "    Returns the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(layers.Dense(dim, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(dim//2, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(dim//4, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_objective(X_train, X_val, y_train, y_val):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates an Optuna objective function to optimize hyperparameters for an MLP model.\n",
    "\n",
    "    The objective trains a model with suggested hyperparameters (layer size, dropout, learning rate),\n",
    "    evaluates it on validation data, and returns the average precision score as the optimization target.\n",
    "    \n",
    "    Returns the objective function to be used in an Optuna study.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        dim = trial.suggest_categorical('dim', [64, 128, 256, 512,1024])\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.6)\n",
    "        learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            model = tf.keras.Sequential([\n",
    "                    tf.keras.Input(shape=(X_train.shape[1],)),\n",
    "                    tf.keras.layers.Dense(dim, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(dim // 2, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(dim // 4, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[early_stopping])\n",
    "            y_pred = model.predict(X_val, verbose = 0).ravel()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Optuna CNN Trial fallito: {e}\")\n",
    "            return 0.0  \n",
    "\n",
    "        return average_precision_score(y_val, y_pred)\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, dim=128, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Creates and compiles a 1D convolutional neural network (CNN) model for binary classification.\n",
    "\n",
    "    The model consists of three Conv1D layers with ReLU activations and max pooling, followed by\n",
    "    a fully connected layer and a sigmoid output layer. Uses Adam optimizer with given learning rate.\n",
    "\n",
    "    Returns the compiled CNN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(layers.Conv1D(dim, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Conv1D(dim//2, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Conv1D(dim//4, 3, activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dim//8, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_objective_cnn(X_train, X_val, y_train, y_val):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates an Optuna objective function to optimize hyperparameters for the 1D CNN model.\n",
    "\n",
    "    The objective trains a CNN with trial-suggested parameters (dim and learning rate),\n",
    "    performs early stopping, and evaluates performance on validation data using average precision score.\n",
    "\n",
    "    Skips trials if input length is too short for the CNN architecture.\n",
    "\n",
    "    Returns the objective function for use in an Optuna study.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        dim = trial.suggest_categorical('dim', [64, 128, 256, 512])\n",
    "        learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "\n",
    "        input_length = X_train.shape[1]\n",
    "      \n",
    "        min_required_length = 3 + 2 + 2  \n",
    "        if input_length < min_required_length:\n",
    "            print(f\"[SKIP TRIAL] Input shape troppo corta: {input_length} (minimo richiesto: {min_required_length})\")\n",
    "            return 0.0 \n",
    "\n",
    "        try:\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.Input(shape=(input_length, 1)),\n",
    "                tf.keras.layers.Conv1D(dim, 3, activation='relu'),\n",
    "                tf.keras.layers.MaxPooling1D(2),\n",
    "                tf.keras.layers.Conv1D(dim//2, 3, activation='relu'),\n",
    "                tf.keras.layers.MaxPooling1D(2),\n",
    "                tf.keras.layers.Conv1D(dim//4, 3, activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(dim//8, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                      validation_data=(X_val, y_val),\n",
    "                      epochs=20,\n",
    "                      batch_size=32,\n",
    "                      verbose=0,\n",
    "                      callbacks=[early_stopping])\n",
    "\n",
    "            y_pred = model.predict(X_val, verbose=0).ravel()\n",
    "            return average_precision_score(y_val, y_pred)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Optuna CNN Trial fallito: {e}\")\n",
    "            return 0.0 \n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modello_deep_learning(best_features, train_set, train_cells, rf_params={}, \n",
    "               base_models=None, param_grid=None, n_splits=5, random_state=4, model_type = 'mlp'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation training of deep learning models (MLP or CNN) on the given dataset.\n",
    "\n",
    "    For each fold:\n",
    "    - Splits data into training and validation based on cell line identifiers.\n",
    "    - Applies SMOTE to balance classes in training data.\n",
    "    - Reshapes data as needed for the selected model type.\n",
    "    - Uses Optuna to optimize hyperparameters (architecture dimensions, dropout, learning rate).\n",
    "    - Trains the model with early stopping on validation loss.\n",
    "    - Evaluates and stores performance metrics (ROC AUC, AUC PR, F1 score).\n",
    "    \n",
    "    Returns a list of trained models with their metrics, and aggregated true and predicted validation labels.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Default model\n",
    "    if base_models is None:\n",
    "        base_models = [create_cnn_model for _ in range(n_splits)]  \n",
    "    \n",
    "    # Kfold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    models_per_fold = []\n",
    "    y_val_all, y_val_pred_all = [], []\n",
    "    i = 0\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_cells):\n",
    "        # Split training and validation\n",
    "        train_cells_fold = train_cells[train_idx]\n",
    "        val_cells_fold = train_cells[val_idx]\n",
    "\n",
    "        train_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(train_cells_fold)]\n",
    "        val_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(val_cells_fold)]\n",
    "\n",
    "        X_train = train_fold[best_features.index.tolist()]\n",
    "        y_train = train_fold[\"Sensitivity\"]\n",
    "\n",
    "        X_val = val_fold[best_features.index.tolist()]\n",
    "        y_val = val_fold[\"Sensitivity\"]\n",
    "\n",
    "\n",
    "        if len(train_fold) == 0 or len(val_fold) == 0:\n",
    "            print(f\"Fold {i}: train or validation fold is empty. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        if model_type == 'cnn':\n",
    "            X_train = X_train.values.reshape(-1, X_train.shape[1], 1)\n",
    "            X_val = X_val.values.reshape(-1, X_val.shape[1], 1)\n",
    "        else: \n",
    "            X_train = X_train.values.astype(\"float32\")\n",
    "            X_val = X_val.values.astype(\"float32\")\n",
    "\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "        # Create models and optuna\n",
    "        if model_type == 'mlp':\n",
    "            objective = create_objective(X_train, X_val, y_train, y_val)\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=10,show_progress_bar=False)\n",
    "\n",
    "            print(\"Best hyperparameters found:\")\n",
    "            print(study.best_params)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            model = create_mlp_model(\n",
    "                input_shape=(X_train.shape[1],),\n",
    "                dim=best_params['dim'],\n",
    "                dropout=best_params['dropout'],\n",
    "                learning_rate=best_params['lr']\n",
    "            )  \n",
    "            \n",
    "        elif model_type == 'cnn':\n",
    "            objective = create_objective_cnn(X_train, X_val, y_train, y_val)\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "            print(\"Best hyperparameters found:\")\n",
    "            print(study.best_params)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            model = create_cnn_model(\n",
    "                input_shape=(X_train.shape[1],1),\n",
    "                dim=best_params['dim'],\n",
    "                learning_rate=best_params['lr']\n",
    "            )\n",
    "\n",
    "        # Fit model\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=20, \n",
    "                  batch_size=min(len(X_train), 32), \n",
    "                  verbose=0, \n",
    "                  validation_data=(X_val, y_val), \n",
    "                  callbacks=[early_stopping])\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_pred_prob = model.predict(X_val, verbose = 0)\n",
    "        y_val_all.extend(y_val)\n",
    "        y_val_pred_all.extend(y_pred_prob)\n",
    "\n",
    "        # Metric score\n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        auc_pr = average_precision_score(y_val, y_pred_prob)\n",
    "        f1 = f1_score(y_val, (y_pred_prob > 0.5).astype(int))\n",
    "        models_per_fold.append({\n",
    "            \"model\": model,\n",
    "            \"auc_score\": auc_score,\n",
    "            \"auc_pr\": auc_pr,\n",
    "            \"f1_score\": f1\n",
    "        })\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # Cleaning for memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    return models_per_fold, y_val_all, y_val_pred_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df_clean, shap_dir, l=31, model_type = 'mlp'):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains predictive models for drug sensitivity.\n",
    "    The function processes multiple drugs in a loop, performing data preprocessing, training, validation, and evaluation \n",
    "    for each drug separately.\n",
    "\n",
    "    Key steps include:\n",
    "    - Loading a previous training state to allow resuming interrupted training\n",
    "    - Splitting data into training and test sets stratified by sensitivity\n",
    "    - Training models with K-fold cross-validation and hyperparameter tuning via Optuna\n",
    "    - Aggregating results and calculating performance metrics such as AUC-ROC, AUC-PR, and F1-score\n",
    "    - Generating and saving plots (precision-recall vs cutoff, confusion matrix, ROC and PR curves)\n",
    "    - Saving intermediate training states for checkpointing and resuming\n",
    "\n",
    "    Parameters:\n",
    "    - df_clean: cleaned input dataframe containing features and drug response labels\n",
    "    - shap_dir: directory path to save results, plots, and model checkpoints\n",
    "    - l: number of drugs to process (default 31)\n",
    "    - model_type: specifies the type of deep learning model ('mlp' or 'cnn')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(shap_dir):\n",
    "        os.makedirs(shap_dir)\n",
    "\n",
    "    label_encoders = {}\n",
    "    df_encoded = df_clean.copy()\n",
    "    df_encoded = df_encoded.dropna(subset=['Sensitivity'])\n",
    "    for col in df_encoded.columns:\n",
    "        if df_encoded[col].dtype == 'object' or df_encoded[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    state_file = os.path.join(shap_dir, \"training_state.pkl\")\n",
    "    state = load_state(state_file)\n",
    "    if state:\n",
    "        print(\"Previous state found. Resuming from where I left off.\")\n",
    "        models_results_rf_specific_drug_sensitivity = state[\"models_results\"]\n",
    "        cumulative_cm = state[\"cumulative_cm\"]\n",
    "        all_fpr_test = state[\"all_fpr_test\"]\n",
    "        all_precision_test = state[\"all_precision_test\"]\n",
    "        drug_names = state[\"drug_names\"]\n",
    "        y_true_total = state[\"y_true_total\"]\n",
    "        y_pred_total = state[\"y_pred_total\"]\n",
    "        start_idx = len(models_results_rf_specific_drug_sensitivity)\n",
    "    else:\n",
    "        models_results_rf_specific_drug_sensitivity = {}\n",
    "        cumulative_cm = np.array([[0, 0], [0, 0]])\n",
    "        all_fpr_test = []\n",
    "        all_precision_test = []\n",
    "        drug_names = []\n",
    "        y_true_total = []\n",
    "        y_pred_total = []\n",
    "        start_idx = 0  \n",
    "\n",
    "\n",
    "    for specific_drug in tqdm(score_df[\"Drug\"].iloc[start_idx+1:l], desc=\"Processing Drugs\"):\n",
    "        # Train model for each drug\n",
    "\n",
    "        print(f\"RAM used: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "        # Directory\n",
    "        models_dir = os.path.join(shap_dir, f\"{drug_name}_models\")\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        \n",
    "        df_drug = df_encoded[df_encoded[\"Drug_id\"] == specific_drug]\n",
    "        drug_name = score_df.loc[score_df[\"Drug\"] == specific_drug, \"Drug_name\"].values[0]\n",
    "        best_features = pd.read_csv(f'Results/Models_Sensitivity/RF/{drug_name}/top20_features.csv', index_col=0)\n",
    "        cell_counts = df_drug[\"Cell_line_cosmic_identifiers\"].value_counts()\n",
    "        train_cells, test_cells = train_test_split(\n",
    "            cell_counts.index,\n",
    "            test_size=0.2,\n",
    "            stratify=df_drug.groupby(\"Cell_line_cosmic_identifiers\")[\"Sensitivity\"].apply(lambda x: x.mean()),\n",
    "            random_state=42\n",
    "        )\n",
    "        train_set = df_drug[df_drug[\"Cell_line_cosmic_identifiers\"].isin(train_cells)]\n",
    "        test_set = df_drug[df_drug[\"Cell_line_cosmic_identifiers\"].isin(test_cells)]\n",
    "        \n",
    "        \n",
    "            \n",
    "        models_per_fold, y_val_all, y_val_pred_all = modello_deep_learning(best_features, train_set, train_cells, rf_params={}, \n",
    "               base_models=None, param_grid=None, n_splits=5, random_state=42 , model_type = model_type)\n",
    "\n",
    "\n",
    "        # Save models\n",
    "        for fold_idx, model_info in enumerate(models_per_fold):\n",
    "            model_path = os.path.join(models_dir, f\"model_fold_{fold_idx}.pkl\")\n",
    "            joblib.dump(model_info, model_path)\n",
    "            print(f\"saved in {model_path}\")\n",
    "\n",
    "        avg_auc_score = np.mean([m[\"auc_score\"] for m in models_per_fold])\n",
    "        avg_auc_pr = np.mean([m[\"auc_pr\"] for m in models_per_fold])\n",
    "        avg_f1_score = np.mean([m[\"f1_score\"] for m in models_per_fold])\n",
    "\n",
    "        X_test = test_set[best_features.index.tolist()]\n",
    "        y_test = test_set[\"Sensitivity\"]\n",
    "\n",
    "        aucprs = np.array([m[\"auc_pr\"] for m in models_per_fold])\n",
    "        weights = aucprs / aucprs.sum()\n",
    "        \n",
    "        \n",
    "        X_test = X_test.values if hasattr(X_test, \"values\") else X_test  \n",
    "        if model_type == 'cnn':\n",
    "                X_test = X_test.reshape(-1, X_test.shape[1], 1)  \n",
    "                y_pred_test_prob = sum(weights[i] * models_per_fold[i][\"model\"].predict(X_test)[:, 0] for i in range(len(weights)))\n",
    "        elif model_type == 'mlp':\n",
    "                X_test = np.array(X_test).astype(\"float32\")\n",
    "                X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "                y_pred_test_prob = sum(\n",
    "                                        weights[i] * models_per_fold[i][\"model\"].predict(X_test)\n",
    "                                        for i in range(len(weights))\n",
    "                                    )\n",
    "                \n",
    "        precision_val, recall_val, thresholds = precision_recall_curve(y_val_all, y_val_pred_all)\n",
    "        f1_scores = 2 * (precision_val * recall_val) / (precision_val + recall_val + 1e-6)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "        y_pred_test = (y_pred_test_prob > best_threshold).astype(int)\n",
    "\n",
    "        auc_score_test = roc_auc_score(y_test, y_pred_test_prob)\n",
    "        auc_pr_test = average_precision_score(y_test, y_pred_test_prob)\n",
    "        f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "        cutoff_range = np.arange(y_pred_test_prob.min(), y_pred_test_prob.max(), 0.01)\n",
    "        precisions_cutoff = []\n",
    "        recalls_cutoff = []\n",
    "\n",
    "        true_labels = np.array(y_test)\n",
    "        predicted_probabilities = np.array(y_pred_test_prob)\n",
    "\n",
    "        for cutoff in cutoff_range:\n",
    "            predicted_labels = (predicted_probabilities > cutoff).astype(int)\n",
    "            precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "            recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "            precisions_cutoff.append(precision)\n",
    "            recalls_cutoff.append(recall)\n",
    "\n",
    "        precision_test_val = precision_score(y_test, y_pred_test)\n",
    "        recall_test_val = recall_score(y_test, y_pred_test)\n",
    "\n",
    "        y_true_total.extend(y_test)\n",
    "        y_pred_total.extend(y_pred_test)\n",
    "\n",
    "        fpr_val, tpr_val, _ = roc_curve(y_val_all, y_val_pred_all)\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_prob)\n",
    "        precision_test, recall_test, _ = precision_recall_curve(y_test, y_pred_test_prob)\n",
    "\n",
    "        all_fpr_test.append((fpr_test, tpr_test))\n",
    "        all_precision_test.append((recall_test, precision_test))\n",
    "        drug_names.append(specific_drug)\n",
    "\n",
    "        # PR cutoff\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.plot(cutoff_range, precisions_cutoff, marker='o', label='Precision', color='blue')\n",
    "        plt.plot(cutoff_range, recalls_cutoff, marker='x', label='Recall', color='orange')\n",
    "        plt.title(f\"Precision & Recall vs Probability Cut-off - {drug_name}\")\n",
    "        plt.xlabel(\"Probability Cut-off\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{shap_dir}/{drug_name}_precision_recall_vs_cutoff.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        cumulative_cm += cm\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "        cm_fig_path = os.path.join(shap_dir, f\"{drug_name}_confusion_matrix.png\")\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix - {drug_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(cm_fig_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Roc curve and PR-curve\n",
    "        roc_fig_path = os.path.join(shap_dir, f\"{drug_name}_roc_curve.png\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fpr_val, tpr_val, label=f'Validation AUC = {avg_auc_score:.3f}')\n",
    "        plt.plot(fpr_test, tpr_test, label=f'Test AUC = {auc_score_test:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - {drug_name}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(recall_val, precision_val, label=f'Validation AUC-PR = {avg_auc_pr:.3f}')\n",
    "        plt.plot(recall_test, precision_test, label=f'Test AUC-PR = {auc_pr_test:.3f}')\n",
    "        plt.axvline(x=best_threshold, color='r', linestyle=\"--\", label=f\"Best Threshold = {best_threshold:.3f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision-Recall Curve - {drug_name}\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Print and save results\n",
    "        models_results_rf_specific_drug_sensitivity[drug_name] = {\n",
    "            \"avg_auc_score\": avg_auc_score,\n",
    "            \"avg_auc_pr\": avg_auc_pr,\n",
    "            \"avg_f1_score\": avg_f1_score,\n",
    "            \"auc_score_test\": auc_score_test,\n",
    "            \"auc_pr_test\": auc_pr_test,\n",
    "            \"f1_score_test\": f1_test,\n",
    "            \"best_threshold\": best_threshold,\n",
    "            \"precision_test\": precision_test_val,\n",
    "            \"recall_test\": recall_test_val\n",
    "        }\n",
    "\n",
    "        print(f\"\\n **Results for {drug_name}** \")\n",
    "        print(f\"AUC-ROC Test: {auc_score_test:.3f}\")\n",
    "        print(f\"AUC-PR Test: {auc_pr_test:.3f}\")\n",
    "        print(f\"F1-score Test: {f1_test:.3f}\")\n",
    "        print(\"\\n **Validation Set Classification Report:**\")\n",
    "        print(classification_report(y_val_all, (np.array(y_val_pred_all) > 0.5).astype(int)))\n",
    "        print(\"\\n**Test Set Classification Report:**\")\n",
    "        print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "        # Save reuslts\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(roc_fig_path)\n",
    "        plt.close()\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        state = {\n",
    "            \"models_results\": models_results_rf_specific_drug_sensitivity,\n",
    "            \"cumulative_cm\": cumulative_cm,\n",
    "            \"all_fpr_test\": all_fpr_test,\n",
    "            \"all_precision_test\": all_precision_test,\n",
    "            \"drug_names\": drug_names,\n",
    "            \"y_true_total\": y_true_total,\n",
    "            \"y_pred_total\": y_pred_total\n",
    "        }\n",
    "        save_state(state_file, state)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\\n========== RIASSUNTO FINALE ==========\")\n",
    "\n",
    "    final_f1 = f1_score(y_true_total, y_pred_total)\n",
    "    results_df = pd.DataFrame.from_dict(models_results_rf_specific_drug_sensitivity, orient=\"index\")\n",
    "    results_df.to_csv(os.path.join(shap_dir, \"Drug_sensitivity_results.csv\"))\n",
    "\n",
    "    cm_fig_path = os.path.join(shap_dir, \"total_confusion_matrix.png\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cumulative_cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "    fig, ax = plt.subplots()\n",
    "    disp.plot(cmap=\"Purples\", ax=ax)\n",
    "    plt.title(f\"Confusion Matrix - TOTALE (Test Set)\\nF1-score Finale: {final_f1:.4f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    roc_all_fig_path = os.path.join(shap_dir, \"roc_curve_all_drugs.png\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, (fpr, tpr) in enumerate(all_fpr_test):\n",
    "        plt.plot(fpr, tpr, label=drug_names[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve - Tutti i farmaci\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(roc_all_fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    pr_all_fig_path = os.path.join(shap_dir, \"precision_recall_curve_all_drugs.png\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, (recall, precision) in enumerate(all_precision_test):\n",
    "        plt.plot(recall, precision, label=drug_names[i])\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve - Tutti i farmaci\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_all_fig_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_Sensitivity/mlp_Sensitivity'\n",
    "\n",
    "train_model(df_clean, shap_dir, l= 31,  model_type = 'mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_Sensitivity/cnn_Sensitivity'\n",
    "\n",
    "train_model(df_clean,shap_dir, l= 31, deeplearning = True, model_type = 'cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model IC50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you can run models that predict IC50 and Drug sensitivity. The analysis specifically focuses on the top 30 drugs, although you can include more if desired. However, note that drugs with a negative score perform worse than a random model and are therefore not recommended for useâ€”this applies especially to drugs ranked beyond the 194th position.\n",
    "\n",
    "The DL functions are implemented here with the teo different approaches (SMOTE+KNN and no oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model_ic50(input_shape, dim=128, dropout=0.2, learning_rate = 0.01):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates and compiles a simple multi-layer perceptron (MLP) model for IC50 regression.\n",
    "    \n",
    "    The architecture includes three hidden layers with ReLU activation, dropout for regularization, \n",
    "    and a final linear output layer. Uses Adam optimizer with specified learning rate.\n",
    "    \n",
    "    Returns the model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(layers.Dense(dim, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(dim//2, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(dim//4, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_objective(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Creates an Optuna objective function to optimize hyperparameters for an MLP model.\n",
    "\n",
    "    The objective trains a model with suggested hyperparameters (layer size, dropout, learning rate),\n",
    "    evaluates it on validation data, and returns the average precision score as the optimization target.\n",
    "    \n",
    "    Returns the objective function to be used in an Optuna study.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        dim = trial.suggest_categorical('dim', [64, 128, 256, 512,1024])\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.6)\n",
    "        learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "\n",
    "        try:\n",
    "\n",
    "            model = tf.keras.Sequential([\n",
    "                    tf.keras.Input(shape=(X_train.shape[1],)),\n",
    "                    tf.keras.layers.Dense(dim, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(dim // 2, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(dim // 4, activation='relu'),\n",
    "                    tf.keras.layers.Dropout(dropout),\n",
    "                    tf.keras.layers.Dense(1, activation='linear')\n",
    "                ])\n",
    "\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[early_stopping])\n",
    "            y_pred = model.predict(X_val, verbose = 0).ravel()\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Optuna CNN Trial fallito: {e}\")\n",
    "            return -1000\n",
    "\n",
    "        return r2\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_ic50(input_shape, dim=128, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Creates and compiles a 1D convolutional neural network (CNN) model for IC50 regression.\n",
    "\n",
    "    The model consists of three Conv1D layers with ReLU activations and max pooling, followed by\n",
    "    a fully connected layer and a sigmoid output layer. Uses Adam optimizer with given learning rate.\n",
    "\n",
    "    Returns the compiled CNN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    model.add(layers.Conv1D(dim, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Conv1D(dim//2, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Conv1D(dim//4, 3, activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dim//8, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_objective_cnn(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Creates an Optuna objective function to optimize hyperparameters for the 1D CNN model.\n",
    "\n",
    "    The objective trains a CNN with trial-suggested parameters (dim and learning rate),\n",
    "    performs early stopping, and evaluates performance on validation data using average precision score.\n",
    "\n",
    "    Skips trials if input length is too short for the CNN architecture.\n",
    "\n",
    "    Returns the objective function for use in an Optuna study.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        dim = trial.suggest_categorical('dim', [64, 128, 256, 512])\n",
    "        learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "\n",
    "        try:\n",
    "\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.Input(shape=(X_train.shape[1], 1)),\n",
    "                tf.keras.layers.Conv1D(dim, 3, activation='relu'),\n",
    "                tf.keras.layers.MaxPooling1D(2),\n",
    "                tf.keras.layers.Conv1D(dim//2, 3, activation='relu'),\n",
    "                tf.keras.layers.MaxPooling1D(2),\n",
    "                tf.keras.layers.Conv1D(dim//4, 3, activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(dim//8, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='linear')\n",
    "            ])\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "            y_pred = model.predict(X_val, verbose=0).ravel()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Optuna CNN Trial fallito: {e}\")\n",
    "            return -1000\n",
    "        \n",
    "        return r2_score(y_val, y_pred)\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modello_deep_learning_ic50(best_features, train_set, train_cells, rf_params={}, \n",
    "               base_models=None, param_grid=None, n_splits=5, random_state=4, model_type = 'mlp', smo = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation training of deep learning models (MLP or CNN) on the given dataset.\n",
    "\n",
    "    For each fold:\n",
    "    - Splits data into training and validation based on cell line identifiers.\n",
    "    - Applies SMOTE if True to balance classes in training data.\n",
    "    - Reshapes data as needed for the selected model type.\n",
    "    - Uses Optuna to optimize hyperparameters (architecture dimensions, dropout, learning rate).\n",
    "    - Trains the model with early stopping on validation loss.\n",
    "    - Evaluates and stores performance metrics (ROC AUC, AUC PR, F1 score).\n",
    "    \n",
    "    Returns a list of trained models with their metrics, and predicted results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base models\n",
    "    if base_models is None:\n",
    "        base_models = [create_mlp_model_ic50 for _ in range(n_splits)]  \n",
    "    \n",
    "    #KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    models_per_fold = []\n",
    "    y_scalers_per_fold = []\n",
    "    y_val_all, y_val_pred_all, y_sens_all = [], [], []\n",
    "    i = 0\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_cells):\n",
    "        # Split training and validation\n",
    "        train_cells_fold = train_cells[train_idx]\n",
    "        val_cells_fold = train_cells[val_idx]\n",
    "        train_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(train_cells_fold)]\n",
    "        val_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(val_cells_fold)]\n",
    "        X_train = train_fold[best_features.index.tolist()]\n",
    "        y_train = train_fold[\"IC50\"]\n",
    "        y_train_sens = train_fold[\"Sensitivity\"]\n",
    "        X_val = val_fold[best_features.index.tolist()]\n",
    "        y_val = val_fold[\"IC50\"]\n",
    "        y_sens = val_fold[\"Sensitivity\"]\n",
    "\n",
    "        if len(train_fold) == 0 or len(val_fold) == 0:\n",
    "            print(f\"Fold {i}: train or validation fold is empty. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if smo:\n",
    "                # SMOTE\n",
    "                smote = SMOTE(random_state=42)\n",
    "                X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_sens)\n",
    "\n",
    "                knn = KNeighborsRegressor(n_neighbors=3)\n",
    "                knn.fit(X_train, y_train)\n",
    "\n",
    "                y_ic50_resampled = knn.predict(X_train_resampled)\n",
    "                df_train_final = X_train_resampled.copy()\n",
    "                df_train_final[\"IC50\"] = y_ic50_resampled\n",
    "                df_train_final[\"Sensitivity\"] = y_train_resampled\n",
    "\n",
    "                X_train_smo = df_train_final.drop(columns=[\"IC50\"])\n",
    "                y_train_ic50_smo = df_train_final[\"IC50\"]\n",
    "\n",
    "                X_train = pd.DataFrame(X_train_smo, columns=X_train.columns)\n",
    "                y_train = pd.Series(y_train_ic50_smo, index=X_train.index)\n",
    "\n",
    "                y_scaler = StandardScaler()\n",
    "                y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "                y_val = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "        else: \n",
    "            y_scaler = StandardScaler()\n",
    "            y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "            y_val = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "        if model_type == 'cnn':\n",
    "            X_train = X_train.values.reshape(-1, X_train.shape[1], 1)\n",
    "            X_val = X_val.values.reshape(-1, X_val.shape[1], 1)\n",
    "        else: \n",
    "            X_train = X_train.values.astype(\"float32\")\n",
    "            X_val = X_val.values.astype(\"float32\")\n",
    "\n",
    "\n",
    "        # Create models and Hyp.Tuning\n",
    "        if model_type == 'mlp':\n",
    "            objective = create_objective(X_train, X_val, y_train, y_val)\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=10)\n",
    "\n",
    "            print(\"Best hyperparameters found:\")\n",
    "            print(study.best_params)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            model = create_mlp_model_ic50(\n",
    "                input_shape=(X_train.shape[1],),\n",
    "                dim=best_params['dim'],\n",
    "                dropout=best_params['dropout'],\n",
    "                learning_rate=best_params['lr']\n",
    "            )\n",
    " \n",
    "        elif model_type == 'cnn':\n",
    "            objective = create_objective_cnn(X_train, X_val, y_train, y_val)\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=10)\n",
    "\n",
    "            print(\"Best hyperparameters found:\")\n",
    "            print(study.best_params)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            model = create_cnn_model_ic50(\n",
    "                input_shape=(X_train.shape[1],1),\n",
    "                dim=best_params['dim'],\n",
    "                learning_rate=best_params['lr']\n",
    "            )\n",
    "\n",
    "            \n",
    "        \n",
    "        # Model fit\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=20, \n",
    "                  batch_size=min(len(X_train), 32), \n",
    "                  verbose=0, \n",
    "                  validation_data=(X_val, y_val), \n",
    "                  callbacks=[early_stopping])\n",
    "        \n",
    "        # Predict IC50\n",
    "        y_pred_ic50 = model.predict(X_val, verbose = 0)\n",
    "        y_pred_ic50 = y_scaler.inverse_transform(y_pred_ic50)\n",
    "        y_scalers_per_fold.append(y_scaler)\n",
    "        y_val_all.extend(y_val)\n",
    "        y_val_pred_all.extend(y_pred_ic50)\n",
    "        y_sens_all.extend(y_sens)\n",
    "\n",
    "        models_per_fold.append(model)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return models_per_fold, y_sens_all, y_val_all, y_val_pred_all,y_scalers_per_fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(best_features, train_set, train_cells,shap_dir, drug_name,model_type, n_splits=5, random_state=4, smo = False):\n",
    "    \"\"\"\n",
    "    Tests previously trained models for a specific drug on validation folds and evaluates their performance.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    models_per_fold = []\n",
    "    y_scalers_per_fold = []\n",
    "    y_val_all, y_val_pred_all, y_sens_all = [], [], []\n",
    "    i = 0\n",
    "    models_dir = os.path.join(shap_dir, f\"{drug_name}_models\")\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_cells):\n",
    "        # Model\n",
    "        model_path = os.path.join(models_dir, f\"model_fold_{i}.pkl\")\n",
    "        model = joblib.load(model_path)\n",
    "        models_per_fold.append(model)\n",
    "\n",
    "        # Split data\n",
    "        train_cells_fold = train_cells[train_idx]\n",
    "        val_cells_fold = train_cells[val_idx]\n",
    "        train_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(train_cells_fold)]\n",
    "        val_fold = train_set[train_set[\"Cell_line_cosmic_identifiers\"].isin(val_cells_fold)]\n",
    "        X_train = train_fold[best_features.index.tolist()]\n",
    "        y_train = train_fold[\"IC50\"]\n",
    "        y_train_sens = train_fold[\"Sensitivity\"]\n",
    "        X_val = val_fold[best_features.index.tolist()]\n",
    "        y_val = val_fold[\"IC50\"]\n",
    "        y_sens = val_fold[\"Sensitivity\"]\n",
    "\n",
    "        if len(train_fold) == 0 or len(val_fold) == 0:\n",
    "            print(f\"Fold {i}: train or validation fold is empty. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if smo:\n",
    "                # SMOTE\n",
    "                smote = SMOTE(random_state=42)\n",
    "                X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_sens)\n",
    "\n",
    "                knn = KNeighborsRegressor(n_neighbors=3)\n",
    "                knn.fit(X_train, y_train)\n",
    "\n",
    "                y_ic50_resampled = knn.predict(X_train_resampled)\n",
    "                df_train_final = X_train_resampled.copy()\n",
    "                df_train_final[\"IC50\"] = y_ic50_resampled\n",
    "                df_train_final[\"Sensitivity\"] = y_train_resampled\n",
    "\n",
    "                X_train_smo = df_train_final.drop(columns=[\"IC50\"])\n",
    "                y_train_ic50_smo = df_train_final[\"IC50\"]\n",
    "\n",
    "                X_train = pd.DataFrame(X_train_smo, columns=X_train.columns)\n",
    "                y_train = pd.Series(y_train_ic50_smo, index=X_train.index)\n",
    "\n",
    "                y_scaler = StandardScaler()\n",
    "                y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "                y_val = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "        else: \n",
    "            y_scaler = StandardScaler()\n",
    "            y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "            y_val = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "        if model_type == 'cnn':\n",
    "            X_train = X_train.values.reshape(-1, X_train.shape[1], 1)\n",
    "            X_val = X_val.values.reshape(-1, X_val.shape[1], 1)\n",
    "        else: \n",
    "            X_train = X_train.values.astype(\"float32\")\n",
    "            X_val = X_val.values.astype(\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "        # Predict IC50\n",
    "        y_pred_ic50 = model.predict(X_val, verbose = 0)\n",
    "        y_pred_ic50 = y_scaler.inverse_transform(y_pred_ic50)\n",
    "        y_scalers_per_fold.append(y_scaler)\n",
    "        y_val_all.extend(y_val)\n",
    "        y_val_pred_all.extend(y_pred_ic50)\n",
    "        y_sens_all.extend(y_sens)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return models_per_fold, y_sens_all, y_val_all, y_val_pred_all,y_scalers_per_fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ic50(df_clean, shap_dir, l=31, model_type = 'mlp', smo = False):\n",
    "    \"\"\"\n",
    "    Trains predictive models for drug IC50 and sensitivity.\n",
    "    The function processes multiple drugs in a loop, performing data preprocessing, training, validation, and evaluation \n",
    "    for each drug separately.\n",
    "\n",
    "    Key steps include:\n",
    "    - Loading a previous training state to allow resuming interrupted training\n",
    "    - Splitting data into training and test sets stratified by sensitivity\n",
    "    - Training models with K-fold cross-validation and hyperparameter tuning via Optuna\n",
    "    - Aggregating results and calculating performance metrics such as AUC-ROC, AUC-PR, and F1-score\n",
    "    - Generating and saving plots (precision-recall vs cutoff, confusion matrix, ROC and PR curves)\n",
    "    - Saving intermediate training states for checkpointing and resuming\n",
    "\n",
    "    Parameters:\n",
    "    - df_clean: cleaned input dataframe containing features and drug response labels\n",
    "    - shap_dir: directory path to save results, plots, and model checkpoints\n",
    "    - l: number of drugs to process (default 31)\n",
    "    - model_type: specifies the type of deep learning model ('mlp' or 'cnn')\n",
    "    - smo True if oversampling\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Directory\n",
    "    if not os.path.exists(shap_dir):\n",
    "        os.makedirs(shap_dir)\n",
    "\n",
    "    label_encoders = {}\n",
    "    df_encoded = df_clean.copy()\n",
    "    for col in df_encoded.columns:\n",
    "        if df_encoded[col].dtype == 'object' or df_encoded[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    df_nan_sens = df_encoded[df_encoded[\"Sensitivity\"].isna()]  \n",
    "    df_encoded = df_encoded[df_encoded[\"Sensitivity\"].notna()]\n",
    "    state_file = os.path.join(shap_dir, \"training_state.pkl\")\n",
    "    state = load_state(state_file)\n",
    "\n",
    "    if not state:\n",
    "        print(\"Stato precedente trovato. Riprendo da dove avevo interrotto.\")\n",
    "        models_results_rf_ic50 = state[\"models_results\"]\n",
    "        y_test_all = state[\"y_test_all\"]\n",
    "        y_pred_all = state[\"y_pred_all\"]\n",
    "        y_test_sens_all = state[\"y_test_sens_all\"]\n",
    "        y_pred_sens_all = state[\"y_pred_sens_all\"]\n",
    "        sensitivity_all = state[\"sensitivity_all\"]\n",
    "        start_idx = len(models_results_rf_ic50)\n",
    "    else:\n",
    "        models_results_rf_ic50 = {}\n",
    "        y_test_all = []\n",
    "        y_pred_all = []\n",
    "        y_test_sens_all =[]\n",
    "        y_pred_sens_all = []\n",
    "        sensitivity_all =[]\n",
    "        start_idx = 0\n",
    "\n",
    "\n",
    "    for specific_drug in tqdm(score_df[\"Drug\"].iloc[start_idx+1:l], desc=\"Processing Drugs\"):\n",
    "        # Model for each drug\n",
    "        df_drug = df_encoded[df_encoded[\"Drug_id\"] == specific_drug]\n",
    "        drug_name = score_df.loc[score_df[\"Drug\"] == specific_drug, \"Drug_name\"].values[0]\n",
    "        best_features = pd.read_csv(f'Results/Models_Sensitivity/RF/{drug_name}/top20_features.csv', index_col=0)\n",
    "\n",
    "        #SPli training and test data\n",
    "        cell_counts = df_drug[\"Cell_line_cosmic_identifiers\"].value_counts()\n",
    "        train_cells, test_cells = train_test_split(\n",
    "            cell_counts.index,\n",
    "            test_size=0.2,\n",
    "            stratify=df_drug.groupby(\"Cell_line_cosmic_identifiers\")[\"Sensitivity\"].apply(lambda x: x.mean()),\n",
    "            random_state=42\n",
    "        )\n",
    "        train_set = df_drug[df_drug[\"Cell_line_cosmic_identifiers\"].isin(train_cells)]\n",
    "        test_set = df_drug[df_drug[\"Cell_line_cosmic_identifiers\"].isin(test_cells)]\n",
    "        X_test = test_set[best_features.index.tolist()]\n",
    "        y_test_ic50 = test_set[\"IC50\"]\n",
    "        sensitivities = test_set[\"Sensitivity\"]\n",
    "\n",
    "        '''\n",
    "        # If you want to train the model again\n",
    "        models_per_fold, y_val_sens, y_val_ic50, y_pred_val,y_scalers_per_fold = modello_deep_learning_ic50(best_features, train_set, train_cells, rf_params={}, \n",
    "               base_models=None, param_grid=None, n_splits=5, random_state=42 , model_type = model_type, smo = smo)\n",
    "\n",
    "\n",
    "        models_dir = os.path.join(shap_dir, f\"{drug_name}_models\")\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        for fold_idx, model_info in enumerate(models_per_fold):\n",
    "            model_path = os.path.join(models_dir, f\"model_fold_{fold_idx}.pkl\")\n",
    "            joblib.dump(model_info, model_path)\n",
    "            print(f\"saved in {model_path}\")\n",
    "            \n",
    "\n",
    "        '''\n",
    "        # Model already trained\n",
    "        models_per_fold, y_val_sens, y_val_ic50, y_pred_val,y_scalers_per_fold = testing(best_features, train_set, train_cells,shap_dir, drug_name,model_type, n_splits=5, random_state=4, smo = False)\n",
    "        \n",
    "\n",
    "        X_test = X_test.values if hasattr(X_test, \"values\") else X_test  \n",
    "        if model_type == 'cnn':\n",
    "                X_test = X_test.reshape(-1, X_test.shape[1], 1)  \n",
    "                y_pred_test_ic50 = sum(1/5 * models_per_fold[i].predict(X_test)[:, 0] for i in range(5))\n",
    "        elif model_type == 'mlp':\n",
    "                X_test = np.array(X_test).astype(\"float32\")\n",
    "                X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "                y_pred_test_ic50 = sum(\n",
    "                                        1/5 * models_per_fold[i].predict(X_test)\n",
    "                                        for i in range(5)\n",
    "                                    )\n",
    "     \n",
    "\n",
    "        # PREDICT IC50\n",
    "        y_pred_test_ic50 = np.mean([m.predict(X_test) for m in models_per_fold], axis=0)\n",
    "        y_pred_test_ic50 = y_scalers_per_fold[0].inverse_transform(y_pred_test_ic50)\n",
    "\n",
    "        # Metrics\n",
    "        mae = mean_absolute_error(y_test_ic50, y_pred_test_ic50)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_ic50, y_pred_test_ic50))\n",
    "        r2 = r2_score(y_test_ic50, y_pred_test_ic50)\n",
    "        f1 = f1_score(true_sensitivity, y_pred_sensitivity)\n",
    "        pearson_corr, _ = pearsonr(y_test_ic50, y_pred_test_ic50.ravel())\n",
    "\n",
    "        df_plot = pd.DataFrame({\n",
    "                                \"True_IC50\": np.array(y_test_ic50).ravel(),\n",
    "                                \"Predicted_IC50\": y_pred_test_ic50.ravel(),\n",
    "                                \"Sensitive\": sensitivities.map({0: \"Non-sensitive\", 1: \"Sensitive\"})\n",
    "                            })\n",
    "        # Cutoff\n",
    "        cutoff_range = np.arange(y_pred_test_ic50.min(), y_pred_test_ic50.max(), 0.1)\n",
    "        precisions_cutoff = []\n",
    "        recalls_cutoff = []\n",
    "        true_sensitivity = np.array(sensitivities)\n",
    "        y_pred_test_ic50 = np.array(y_pred_test_ic50)\n",
    "\n",
    "        for cutoff in cutoff_range:\n",
    "            predicted_sensitivity = (y_pred_test_ic50 < cutoff).astype(int)\n",
    "            precision = precision_score(true_sensitivity, predicted_sensitivity, zero_division=0)\n",
    "            recall = recall_score(true_sensitivity, predicted_sensitivity, zero_division=0)\n",
    "\n",
    "            precisions_cutoff.append(precision)\n",
    "            recalls_cutoff.append(recall)\n",
    "\n",
    "        # Best threshold \n",
    "        y_score_test = -y_pred_test_ic50  \n",
    "        y_score_val =  -np.array(y_pred_val)\n",
    "        fpr_test, tpr_test, roc_thresholds = roc_curve(sensitivities, y_score_test)\n",
    "        fpr_val, tpr_val, roc_thresholds_val = roc_curve(y_val_sens, y_score_val)\n",
    "        auc_score_test = roc_auc_score(sensitivities, y_score_test)\n",
    "        auc_score_val = roc_auc_score(y_val_sens, y_score_val)\n",
    "        precision_test, recall_test, pr_thresholds = precision_recall_curve(sensitivities, y_score_test)\n",
    "        avg_auc_pr_test = average_precision_score(sensitivities, y_score_test)\n",
    "        precision_val, recall_val, pr_thresholds_val = precision_recall_curve(y_val_sens, y_score_val)\n",
    "        avg_auc_pr_val = average_precision_score(y_val_sens, y_score_val)\n",
    "        f1_scores_val = 2 * (precision_val[:-1] * recall_val[:-1]) / (precision_val[:-1] + recall_val[:-1] + 1e-8)\n",
    "        best_idx_val = np.argmax(f1_scores_val)\n",
    "        best_threshold_val = -pr_thresholds_val[best_idx_val]\n",
    "        y_pred_sensitivity = (y_pred_test_ic50 <= best_threshold_val).astype(int)\n",
    "\n",
    "\n",
    "        y_test_all.extend(y_test_ic50)\n",
    "        y_pred_all.extend(y_pred_test_ic50)\n",
    "        y_test_sens_all.extend(true_sensitivity)\n",
    "        y_pred_sens_all.extend(y_pred_sensitivity)\n",
    "        sensitivity_all.extend(sensitivities)\n",
    "\n",
    "        # Save results\n",
    "        models_results_rf_ic50[specific_drug] = {\n",
    "                \"MAE_IC50\": mae,\n",
    "                \"RMSE_IC50\": rmse,\n",
    "                \"R2_IC50\": r2,\n",
    "                \"Pearson\": pearson_corr,\n",
    "                \"F1-score\": f1,\n",
    "                \"ROC_AUC\": auc_score_test,\n",
    "                \"PR_AUC\": avg_auc_pr_test,\n",
    "            }\n",
    "        \n",
    "\n",
    "        # PLot IC50\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.scatterplot(data=df_plot, x=\"True_IC50\", y=\"Predicted_IC50\", hue=\"Sensitive\", alpha=0.7)\n",
    "        plt.plot([df_plot[\"True_IC50\"].min(), df_plot[\"True_IC50\"].max()],\n",
    "                [df_plot[\"True_IC50\"].min(), df_plot[\"True_IC50\"].max()], 'r--')\n",
    "        plt.xlabel(\"True IC50\")\n",
    "        plt.ylabel(\"Predicted IC50\")\n",
    "        plt.title(f\"IC50 Prediction- {drug_name}\")\n",
    "        plt.legend(title=\"Cell Sensitivity\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{shap_dir}/{drug_name}_ic50_scatter_rf.png\")\n",
    "        plt.close()\n",
    "\n",
    "        #PR cutoff\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.plot(cutoff_range, precisions_cutoff, marker='o', label='Precision', color='blue')\n",
    "        plt.plot(cutoff_range, recalls_cutoff, marker='x', label='Recall', color='orange')\n",
    "        plt.title(f\"Precision & Recall vs IC50 Cut-off - {drug_name}\")\n",
    "        plt.xlabel(\"IC50 Cut-off (on predicted values)\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{shap_dir}/{drug_name}_precision_recall_curve.png\")\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_sensitivity, y_pred_sensitivity)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Non-sensitive\", \"Sensitive\"], yticklabels=[\"Non-sensitive\", \"Sensitive\"])\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(f\"Confusion Matrix with cutoff- {drug_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{shap_dir}/{drug_name}_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # ROc curve\n",
    "        roc_fig_path = os.path.join(shap_dir, f\"{drug_name}_roc_curve.png\")\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fpr_val, tpr_val, label=f'Validation AUC = {auc_score_test:.3f}')\n",
    "        plt.plot(fpr_test, tpr_test, label=f'Test AUC = {auc_score_test:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - {drug_name}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(recall_val, precision_val, label=f'Validation AUC-PR = {avg_auc_pr_val:.3f}')\n",
    "        plt.plot(recall_test, precision_test, label=f'Test AUC-PR = {avg_auc_pr_test:.3f}')\n",
    "        plt.axvline(x=recall_val[best_idx_val], color='r', linestyle=\"--\", label=f\"Best Threshold = {best_threshold_val:.3f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision-Recall Curve - {drug_name}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(roc_fig_path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Print and save results\n",
    "        print(f\"\\n** Results for {drug_name} **\")\n",
    "        print(f\"IC50 - MAE: {mae:.3f}, RMSE: {rmse:.3f}, RÂ²: {r2:.3f}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        state = {\n",
    "            \"models_results\": models_results_rf_ic50,\n",
    "            \"y_test_all\" :y_test_all,\n",
    "            \"y_pred_all\":y_pred_all,\n",
    "            \"y_test_sens_all\" :y_test_sens_all,\n",
    "            \"y_pred_sens_all\":y_pred_sens_all,\n",
    "            \"sensitivity_all\" : sensitivity_all,\n",
    "            \"start_idx\" : start_idx\n",
    "        }\n",
    "        save_state(state_file, state)\n",
    "\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(models_results_rf_ic50, orient=\"index\")\n",
    "    results_df.to_csv(os.path.join(shap_dir, \"rf_drug_sensitivity_results.csv\"))\n",
    "\n",
    "    # Final metrics\n",
    "    mae_global = mean_absolute_error(y_test_all, y_pred_all)\n",
    "    rmse_global = np.sqrt(mean_squared_error(y_test_all, y_pred_all))\n",
    "    r2_global = r2_score(y_test_all, y_pred_all)\n",
    "    final_f1 = f1_score(y_test_sens_all, y_pred_sens_all)\n",
    "    cm_global = confusion_matrix(y_test_sens_all, y_pred_sens_all)\n",
    "\n",
    "    for i, val in enumerate(y_test_sens_all):\n",
    "        if not np.isscalar(val):\n",
    "            print(f\"Non-scalar at index {i}: {val} ({type(val)})\")\n",
    "\n",
    "    df_global = pd.DataFrame({\n",
    "        \"True_IC50\": y_test_all,\n",
    "        \"Predicted_IC50\": y_pred_all,\n",
    "        \"Sensitive\": pd.Series([int(x) for x in y_test_sens_all]).map({0: \"Non-sensitive\", 1: \"Sensitive\"})\n",
    "\n",
    "    })\n",
    "\n",
    "    df_global[\"Predicted_IC50\"] = df_global[\"Predicted_IC50\"].apply(lambda x: float(x[0]) if isinstance(x, (np.ndarray, list)) else float(x))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\\n========= GLOBAL METRICS ACROSS ALL DRUGS =========\")\n",
    "    print(f\"Global IC50 - MAE: {mae_global:.3f}, RMSE: {rmse_global:.3f}, RÂ²: {r2_global:.3f}\")\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm_global, annot=True, fmt='d', cmap='Purples', xticklabels=[\"Non-sensitive\", \"Sensitive\"], yticklabels=[\"Non-sensitive\", \"Sensitive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Global Confusion Matrix (All Drugs) with f1: {final_f1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{shap_dir}/global_confusion_matrix_rf.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #FInal IC50\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sns.scatterplot(data=df_global, x=\"True_IC50\", y=\"Predicted_IC50\", hue=\"Sensitive\", alpha=0.5)\n",
    "    plt.plot([df_global[\"True_IC50\"].min(), df_global[\"True_IC50\"].max()],\n",
    "            [df_global[\"True_IC50\"].min(), df_global[\"True_IC50\"].max()], 'r--')\n",
    "    plt.xlabel(\"True IC50\")\n",
    "    plt.ylabel(\"Predicted IC50\")\n",
    "    plt.title(f\"Global IC50 Prediction Across All Drugs RÂ²: {r2_global:.3f}\")\n",
    "    plt.legend(title=\"Cell Sensitivity\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{shap_dir}/global_ic50_prediction_rf.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO versampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_IC50/cnn'\n",
    "\n",
    "train_model_ic50(df_clean, shap_dir, l= 31, model_type = 'cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_IC50/mlp'\n",
    "\n",
    "train_model_ic50(df_clean,shap_dir, l= 31, model_type = 'mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_IC50_SMOTE/mlp'\n",
    "\n",
    "train_model_ic50(df_clean,shap_dir, l= 31, model_type = 'mlp', smo = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_dir = 'Results/Models_IC50_SMOTE/cnn'\n",
    "\n",
    "train_model_ic50(df_clean,shap_dir, l= 31,  model_type = 'cnn', smo = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
